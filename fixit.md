# üîß XALQ Agent - An√°lise de Melhorias (Baseada no C√≥digo Real)

**Data da An√°lise:** 12 de fevereiro de 2026  
**Vers√£o Atual:** 1.2.0  
**An√°lise:** C√≥digo-fonte completo verificado

---

## ÔøΩ Status Geral do Projeto

### ‚úÖ O Que J√Å Est√° Implementado

**Seguran√ßa:**
- ‚úÖ Sanitiza√ß√£o de API Keys em logs (parcial - apenas chaves pr√≥prias)
- ‚úÖ Keyring para armazenamento seguro de credenciais
- ‚úÖ Sanitiza√ß√£o b√°sica de nomes de arquivo

**API & Resili√™ncia:**
- ‚úÖ Retry com backoff exponencial (via `tenacity`)
- ‚úÖ Fallback autom√°tico entre modelos Gemini
- ‚úÖ Timeout configur√°vel (600s)
- ‚úÖ Suporte a cancelamento de processamento

**UI/UX:**
- ‚úÖ Drag & Drop para arquivos
- ‚úÖ Bot√£o de cancelar processamento
- ‚úÖ Monitor de recursos (CPU/RAM)
- ‚úÖ Timer de elapsed time
- ‚úÖ Splash screen com branding

**Qualidade:**
- ‚úÖ Testes b√°sicos implementados (sanitiza√ß√£o, logs)
- ‚úÖ Logging estruturado com n√≠veis
- ‚úÖ Tratamento de erros espec√≠ficos

**Infraestrutura:**
- ‚úÖ Depend√™ncias gerenciadas (requirements.txt)
- ‚úÖ Launcher com auto-instala√ß√£o
- ‚úÖ Sistema de versionamento

---

## üéØ Resumo Executivo

An√°lise do c√≥digo-fonte identificou **32 melhorias pendentes** distribu√≠das em 6 categorias. O projeto j√° possui uma base s√≥lida com seguran√ßa b√°sica, retry autom√°tico e UI moderna. Foco deve ser em: cobertura de testes, otimiza√ß√£o de custos API, e features avan√ßadas de UX.

### ÔøΩ Distribui√ß√£o de Melhorias Pendentes

```
Testes & Qualidade    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 8 itens  (25%)  üî¥ Alta Prioridade
API & Custos          ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 6 itens    (19%)  üî¥ Alta Prioridade
UX/UI Avan√ßada        ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 8 itens  (25%)  üü° M√©dia Prioridade
Performance           ‚ñà‚ñà‚ñà‚ñà 4 itens      (12%)  üü° M√©dia Prioridade
DevOps & CI/CD        ‚ñà‚ñà‚ñà‚ñà 4 itens      (12%)  üü¢ Baixa Prioridade
Observabilidade       ‚ñà‚ñà 2 itens        (7%)   üü¢ Baixa Prioridade
```

### üéØ Top 5 Prioridades Imediatas

1. üí∞ **Monitoramento de Custos API** - Sem tracking de gastos com Gemini
2. üß™ **Expandir Cobertura de Testes** - Apenas 2 testes b√°sicos (5% cobertura)
3. üíæ **Cache de Respostas API** - Economia de 30% em reprocessamentos
4. üìä **Rate Limiting** - Pode exceder limites da API (15 RPM free)
5. üîç **Busca em Logs** - Dif√≠cil encontrar erros em logs longos

---

## üî¥ ALTA PRIORIDADE - Testes & Qualidade

### 1. Cobertura de Testes Insuficiente
**Status Atual:** 2 testes b√°sicos em `test_worker_engine.py` (~5% cobertura)  
**Problema:** Fun√ß√µes cr√≠ticas sem testes (parse_response, call_ai_api, generate_word_report)  
**Impacto:** Bugs em produ√ß√£o, refatora√ß√£o arriscada

**Solu√ß√£o:** Expandir suite de testes

```python
# tests/test_worker_engine.py (adicionar)

def test_parse_response_strict_format(mock_engine):
    """Test parsing with [SECTION]...[/SECTION] format"""
    response = "[RESUMO_EXECUTIVO]Test Summary[/RESUMO_EXECUTIVO][DIAGNOSTICO]Test Diag[/DIAGNOSTICO]"
    parsed = mock_engine.parse_response(response)
    assert parsed["RESUMO_EXECUTIVO"] == "Test Summary"
    assert parsed["DIAGNOSTICO"] == "Test Diag"

def test_parse_response_flexible_fallback(mock_engine):
    """Test fallback parsing when strict format fails"""
    response = "RESUMO EXECUTIVO\nTest content\n\nDIAGNOSTICO\nMore content"
    parsed = mock_engine.parse_response(response)
    assert "Test content" in parsed["RESUMO_EXECUTIVO"]

def test_load_data_csv(mock_engine, tmp_path):
    """Test CSV loading"""
    csv_file = tmp_path / "test.csv"
    csv_file.write_text("nome da empresa,valor\nEmpresa A,100\nEmpresa B,200")
    df, items = mock_engine.load_data(str(csv_file))
    assert len(df) == 2
    assert len(items) == 2
    assert "Empresa A" in items[0]

def test_load_data_excel(mock_engine, tmp_path):
    """Test Excel loading"""
    import pandas as pd
    excel_file = tmp_path / "test.xlsx"
    df = pd.DataFrame({"nome da empresa": ["Empresa A", "Empresa B"], "valor": [100, 200]})
    df.to_excel(excel_file, index=False)
    loaded_df, items = mock_engine.load_data(str(excel_file))
    assert len(loaded_df) == 2

@patch('core.worker_engine.genai.GenerativeModel')
def test_call_ai_api_success(mock_model_class, mock_engine):
    """Test successful API call"""
    mock_model = MagicMock()
    mock_response = MagicMock()
    mock_response.parts = ["content"]
    mock_response.text = "AI Response"
    mock_model.generate_content.return_value = mock_response
    mock_model_class.return_value = mock_model
    
    result = mock_engine.call_ai_api("test prompt", {"model": "gemini-3-pro-preview"})
    assert result == "AI Response"

@patch('core.worker_engine.genai.GenerativeModel')
def test_call_ai_api_fallback(mock_model_class, mock_engine):
    """Test fallback to alternative model on failure"""
    mock_model = MagicMock()
    # First model fails
    mock_model.generate_content.side_effect = [Exception("404"), MagicMock(parts=["ok"], text="Success")]
    mock_model_class.return_value = mock_model
    
    result = mock_engine.call_ai_api("test prompt", {"model": "invalid-model"})
    assert result == "Success"
```

**Meta:** 70% de cobertura de c√≥digo

---

### 2. Valida√ß√£o de Resposta da API Ausente
**Status Atual:** N√£o implementado  
**Problema:** Respostas incompletas ou malformadas n√£o s√£o detectadas antes de gerar DOCX  
**Impacto:** Relat√≥rios vazios ou incompletos

**Solu√ß√£o:**

```python
# core/worker_engine.py (adicionar m√©todo)

def validate_ai_response(self, response_text):
    """Valida se a resposta cont√©m se√ß√µes m√≠nimas esperadas"""
    if not response_text or len(response_text) < 100:
        return False, "Resposta muito curta (< 100 caracteres)"
    
    required_sections = ["RESUMO_EXECUTIVO", "DIAGNOSTICO", "PROXIMOS_PASSOS"]
    missing = []
    
    for section in required_sections:
        # Check both strict and flexible formats
        if f"[{section}]" not in response_text and section.replace('_', ' ') not in response_text.upper():
            missing.append(section)
    
    if missing:
        return False, f"Se√ß√µes obrigat√≥rias faltando: {', '.join(missing)}"
    
    return True, "OK"

# Modificar call_ai_api para usar valida√ß√£o:
def call_ai_api(self, prompt_content, config):
    # ... c√≥digo existente ...
    
    response_text = response.text
    
    # Validar resposta
    is_valid, message = self.validate_ai_response(response_text)
    if not is_valid:
        self.log_and_progress(
            f"‚ö†Ô∏è Resposta inv√°lida: {message}. Tentando pr√≥ximo modelo...",
            "error"
        )
        continue  # Tenta pr√≥ximo modelo no loop
    
    return response_text
```

---

### 3. Tratamento de Erros Gen√©rico em Alguns Locais
**Status Atual:** Melhorado mas ainda h√° `except Exception` em alguns lugares  
**Problema:** Dificulta debug de erros espec√≠ficos  
**Impacto:** Tempo maior para resolver problemas

**Solu√ß√£o:** Refinar tratamento de erros

```python
# core/worker_engine.py - Melhorar load_data

def load_data(self, file_path):
    import pandas as pd
    try:
        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path, encoding='utf-8')
        elif file_path.endswith(('.xlsx', '.xls')):
            df = pd.read_excel(file_path)
        else:
            self.log_and_progress(f"Formato n√£o suportado: {file_path}", "error")
            return None, []
        
        if df.empty:
            self.log_and_progress("Arquivo vazio", "error")
            return None, []
        
        # ... resto do c√≥digo ...
        
    except FileNotFoundError:
        self.log_and_progress(f"Arquivo n√£o encontrado: {file_path}", "error")
        return None, []
    except pd.errors.EmptyDataError:
        self.log_and_progress("Arquivo CSV vazio ou malformado", "error")
        return None, []
    except pd.errors.ParserError as e:
        self.log_and_progress(f"Erro ao parsear arquivo: {e}", "error")
        return None, []
    except PermissionError:
        self.log_and_progress(f"Sem permiss√£o para ler: {file_path}", "error")
        return None, []
    except Exception as e:
        self.logger.exception("Erro inesperado ao carregar dados:")
        self.log_and_progress(f"Erro ao carregar dados: {e}", "error")
        return None, []
```

---

## üî¥ ALTA PRIORIDADE - API & Custos

### 4. Aus√™ncia de Monitoramento de Custos
**Status Atual:** N√£o implementado  
**Problema:** Sem tracking de tokens e custos acumulados  
**Impacto:** Gastos descontrolados, sem visibilidade financeira

**Solu√ß√£o:**

```python
# core/cost_tracker.py (novo arquivo)

import json
from datetime import datetime
from pathlib import Path

class CostTracker:
    # Pre√ßos por 1M tokens (fev 2026)
    PRICING = {
        'gemini-3-pro-preview': {'input': 1.25, 'output': 5.00},
        'gemini-2.5-pro': {'input': 1.25, 'output': 5.00},
        'gemini-2.5-flash': {'input': 0.075, 'output': 0.30},
        'gemini-2.0-flash': {'input': 0.075, 'output': 0.30},
    }
    
    def __init__(self, log_file='logs/costs.json'):
        self.log_file = Path(log_file)
        self.log_file.parent.mkdir(exist_ok=True)
        self.session_costs = []
    
    def log_request(self, model, input_tokens, output_tokens):
        # Normalize model name
        model_key = model.replace('models/', '')
        if model_key not in self.PRICING:
            # Try to find closest match
            for key in self.PRICING:
                if key in model_key:
                    model_key = key
                    break
            else:
                return None
        
        pricing = self.PRICING[model_key]
        input_cost = (input_tokens / 1_000_000) * pricing['input']
        output_cost = (output_tokens / 1_000_000) * pricing['output']
        total_cost = input_cost + output_cost
        
        entry = {
            'timestamp': datetime.now().isoformat(),
            'model': model,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'cost_usd': round(total_cost, 4)
        }
        
        self.session_costs.append(entry)
        self._save_to_file(entry)
        
        return total_cost
    
    def get_session_total(self):
        return sum(e['cost_usd'] for e in self.session_costs)
    
    def get_session_stats(self):
        if not self.session_costs:
            return None
        
        return {
            'total_requests': len(self.session_costs),
            'total_tokens': sum(e['input_tokens'] + e['output_tokens'] for e in self.session_costs),
            'total_cost': self.get_session_total(),
            'avg_cost_per_request': self.get_session_total() / len(self.session_costs)
        }
    
    def _save_to_file(self, entry):
        try:
            with open(self.log_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(entry) + '\n')
        except Exception:
            pass

# core/worker_engine.py - Integrar no __init__

from core.cost_tracker import CostTracker

def __init__(self, ...):
    # ... c√≥digo existente ...
    self.cost_tracker = CostTracker()

# Modificar call_ai_api para logar custos:

def call_ai_api(self, prompt_content, config):
    # ... c√≥digo existente ...
    
    response = model.generate_content(...)
    
    # Extrair metadata de uso
    if hasattr(response, 'usage_metadata') and response.usage_metadata:
        usage = response.usage_metadata
        cost = self.cost_tracker.log_request(
            model_name,
            usage.prompt_token_count,
            usage.candidates_token_count
        )
        
        if cost:
            self.log_and_progress(
                f"üí∞ Custo: ${cost:.4f} USD "
                f"({usage.prompt_token_count + usage.candidates_token_count} tokens)",
                "info"
            )
            
            stats = self.cost_tracker.get_session_stats()
            if stats:
                self.log_and_progress(
                    f"üìä Sess√£o: {stats['total_requests']} req, "
                    f"${stats['total_cost']:.2f} USD total",
                    "debug"
                )
    
    return response.text
```

**Economia Estimada:** Visibilidade para otimizar e economizar ~20-30% em custos

---

### 5. Aus√™ncia de Cache de Respostas
**Status Atual:** N√£o implementado  
**Problema:** Mesmos dados reprocessados custam dinheiro  
**Impacto:** Desperd√≠cio de ~30% em reprocessamentos

**Solu√ß√£o:**

```python
# core/response_cache.py (novo arquivo)

import hashlib
import json
from pathlib import Path
from datetime import datetime, timedelta

class ResponseCache:
    def __init__(self, cache_dir='.cache/api_responses', ttl_days=7):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.ttl = timedelta(days=ttl_days)
    
    def _get_hash(self, prompt, model, config):
        """Gera hash √∫nico para prompt + configura√ß√£o"""
        data = {
            'prompt': prompt[:1000],  # Primeiros 1000 chars para evitar hash muito longo
            'model': model,
            'temperature': config.get('temperature'),
            'top_p': config.get('top_p', 0.9)
        }
        content = json.dumps(data, sort_keys=True)
        return hashlib.sha256(content.encode()).hexdigest()
    
    def get(self, prompt, model, config):
        cache_key = self._get_hash(prompt, model, config)
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        if cache_file.exists():
            try:
                with open(cache_file, 'r', encoding='utf-8') as f:
                    cached = json.load(f)
                    cached_time = datetime.fromisoformat(cached['timestamp'])
                    
                    if datetime.now() - cached_time < self.ttl:
                        return cached['response']
                    else:
                        # Cache expirado, deletar
                        cache_file.unlink()
            except Exception:
                pass
        return None
    
    def set(self, prompt, model, config, response):
        cache_key = self._get_hash(prompt, model, config)
        cache_file = self.cache_dir / f"{cache_key}.json"
        
        data = {
            'timestamp': datetime.now().isoformat(),
            'model': model,
            'response': response
        }
        
        try:
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        except Exception:
            pass
    
    def clear_expired(self):
        """Remove caches expirados"""
        count = 0
        for cache_file in self.cache_dir.glob("*.json"):
            try:
                with open(cache_file, 'r') as f:
                    cached = json.load(f)
                    cached_time = datetime.fromisoformat(cached['timestamp'])
                    if datetime.now() - cached_time >= self.ttl:
                        cache_file.unlink()
                        count += 1
            except:
                pass
        return count

# core/worker_engine.py - Integrar

from core.response_cache import ResponseCache

def __init__(self, ...):
    # ... c√≥digo existente ...
    self.response_cache = ResponseCache(ttl_days=7)

def call_ai_api(self, prompt_content, config):
    model_name = config.get('model', 'gemini-3-pro-preview')
    
    # Tentar cache primeiro
    cached = self.response_cache.get(prompt_content, model_name, config)
    if cached:
        self.log_and_progress(
            f"üíæ Resposta encontrada em cache! Economia de tempo e custo.",
            "success"
        )
        return cached
    
    # ... chamada API normal ...
    
    response_text = response.text
    
    # Salvar em cache
    if response_text:
        self.response_cache.set(prompt_content, model_name, config, response_text)
    
    return response_text
```

**Economia Estimada:** $45/m√™s (30% de reprocessamentos evitados)

---

### 6. Aus√™ncia de Rate Limiting
**Status Atual:** N√£o implementado  
**Problema:** M√∫ltiplas requisi√ß√µes simult√¢neas podem exceder limites (15 RPM free, 360 RPM paid)  
**Impacto:** Bloqueios tempor√°rios, erros 429

**Solu√ß√£o:**

```python
# core/rate_limiter.py (novo arquivo)

import threading
from collections import deque
from datetime import datetime, timedelta
import time

class RateLimiter:
    def __init__(self, max_requests=15, time_window=60):
        """
        max_requests: n√∫mero m√°ximo de requisi√ß√µes
        time_window: janela de tempo em segundos
        """
        self.max_requests = max_requests
        self.time_window = timedelta(seconds=time_window)
        self.requests = deque()
        self.lock = threading.Lock()
    
    def acquire(self, blocking=True):
        """
        Tenta adquirir permiss√£o para fazer requisi√ß√£o.
        Se blocking=True, aguarda at√© poder fazer.
        Retorna: (can_proceed, wait_time)
        """
        with self.lock:
            now = datetime.now()
            
            # Remove requisi√ß√µes antigas
            while self.requests and now - self.requests[0] > self.time_window:
                self.requests.popleft()
            
            # Verifica se pode fazer requisi√ß√£o
            if len(self.requests) >= self.max_requests:
                if not blocking:
                    oldest = self.requests[0]
                    wait_time = (oldest + self.time_window - now).total_seconds()
                    return False, wait_time
                
                # Blocking: aguardar
                oldest = self.requests[0]
                wait_time = (oldest + self.time_window - now).total_seconds()
                
                if wait_time > 0:
                    return False, wait_time
            
            self.requests.append(now)
            return True, 0

# core/worker_engine.py - Integrar

from core.rate_limiter import RateLimiter

def __init__(self, ...):
    # ... c√≥digo existente ...
    # Gemini Free: 15 RPM, Paid: 360 RPM
    # Detectar tier automaticamente ou usar configura√ß√£o
    rpm_limit = int(os.environ.get("GEMINI_RPM_LIMIT", "15"))
    self.rate_limiter = RateLimiter(max_requests=rpm_limit, time_window=60)

def call_ai_api(self, prompt_content, config):
    # Aguardar rate limit
    can_proceed, wait_time = self.rate_limiter.acquire(blocking=False)
    if not can_proceed:
        self.log_and_progress(
            f"‚è≥ Rate limit atingido. Aguardando {wait_time:.1f}s...",
            "warning"
        )
        time.sleep(wait_time + 0.1)  # +0.1s de margem
        self.rate_limiter.acquire(blocking=True)
    
    # ... resto do c√≥digo ...
```

---

## üü° M√âDIA PRIORIDADE - UX/UI Avan√ßada

### 7. Aus√™ncia de Busca e Filtros em Logs
**Status Atual:** N√£o implementado  
**Problema:** Dif√≠cil encontrar mensagens espec√≠ficas em logs longos  
**Impacto:** Tempo perdido procurando erros

**Solu√ß√£o:**

```python
# ui/main_window.py - Adicionar na setup_ui, antes do log_area

log_toolbar = QHBoxLayout()

self.log_search = QLineEdit()
self.log_search.setPlaceholderText("üîç Buscar nos logs...")
self.log_search.textChanged.connect(self.filter_logs)
self.log_search.setMaximumWidth(300)

self.log_filter = QComboBox()
self.log_filter.addItems(["Todos", "Erros", "Sucesso", "Info", "Debug"])
self.log_filter.currentTextChanged.connect(self.filter_logs)
self.log_filter.setMaximumWidth(150)

btn_clear_logs = QPushButton("üóëÔ∏è Limpar")
btn_clear_logs.clicked.connect(self.log_area.clear)
btn_clear_logs.setMaximumWidth(100)

btn_export_logs = QPushButton("üíæ Exportar")
btn_export_logs.clicked.connect(self.export_logs)
btn_export_logs.setMaximumWidth(100)

log_toolbar.addWidget(self.log_search)
log_toolbar.addWidget(self.log_filter)
log_toolbar.addWidget(btn_clear_logs)
log_toolbar.addWidget(btn_export_logs)
log_toolbar.addStretch()

content_layout.addLayout(log_toolbar)

# Adicionar m√©todos:

def filter_logs(self):
    search_text = self.log_search.text().lower()
    filter_type = self.log_filter.currentText()
    
    # Store original logs if not stored
    if not hasattr(self, '_all_logs'):
        self._all_logs = []
    
    # Apply filters
    self.log_area.clear()
    for log_entry in self._all_logs:
        text, level = log_entry
        
        # Filter by type
        if filter_type != "Todos":
            if filter_type == "Erros" and level != "error":
                continue
            elif filter_type == "Sucesso" and level != "success":
                continue
            elif filter_type == "Info" and level != "info":
                continue
            elif filter_type == "Debug" and level != "debug":
                continue
        
        # Filter by search
        if search_text and search_text not in text.lower():
            continue
        
        # Add to display
        self._add_log_to_display(text, level)

def log(self, message, level="info"):
    # Store in memory
    if not hasattr(self, '_all_logs'):
        self._all_logs = []
    self._all_logs.append((message, level))
    
    # Check if should display based on current filters
    search_text = self.log_search.text().lower() if hasattr(self, 'log_search') else ""
    filter_type = self.log_filter.currentText() if hasattr(self, 'log_filter') else "Todos"
    
    if filter_type != "Todos":
        if filter_type == "Erros" and level != "error":
            return
        elif filter_type == "Sucesso" and level != "success":
            return
        elif filter_type == "Info" and level != "info":
            return
        elif filter_type == "Debug" and level != "debug":
            return
    
    if search_text and search_text not in message.lower():
        return
    
    self._add_log_to_display(message, level)

def _add_log_to_display(self, message, level):
    from datetime import datetime
    timestamp = datetime.now().strftime("%H:%M:%S")
    
    color = "#8CC63F"
    if level == "error":
        color = "#EF4444"
    elif level == "success":
        color = "#34D399"
    elif level == "highlight":
        color = "#60A5FA"
    elif level == "debug":
        color = "#94A3B8"
    
    html = f'<span style="color:#64748B;">[{timestamp}]</span> <span style="color:{color};">{message}</span>'
    self.log_area.append(html)
    sb = self.log_area.verticalScrollBar()
    sb.setValue(sb.maximum())

def export_logs(self):
    from datetime import datetime
    file_path, _ = QFileDialog.getSaveFileName(
        self, "Exportar Logs", 
        f"logs_xalq_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
        "Text Files (*.txt);;All Files (*)"
    )
    
    if file_path:
        try:
            import re
            plain_text = self.log_area.toPlainText()
            
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(plain_text)
            
            self.log(f"‚úÖ Logs exportados para: {file_path}", "success")
        except Exception as e:
            QMessageBox.critical(self, "Erro", f"Falha ao exportar logs: {e}")
```

---

### 8. Aus√™ncia de Preview de Dados
**Status Atual:** N√£o implementado  
**Problema:** Usu√°rio n√£o v√™ dados antes de processar  
**Impacto:** Erros de arquivo n√£o detectados

**Solu√ß√£o:**

```python
# ui/main_window.py - Adicionar bot√£o de preview

# No setup_ui, modificar file_row:
btn_preview = QPushButton("üëÅÔ∏è Preview")
btn_preview.clicked.connect(self.show_file_preview)
file_row.addWidget(btn_preview)

# Adicionar m√©todo:

def show_file_preview(self):
    file_path = self.file_path_input.text()
    if not file_path:
        QMessageBox.warning(self, "Aviso", "Selecione um arquivo primeiro.")
        return
    
    try:
        df, _ = self.worker_engine.load_data(file_path)
        if df is None:
            QMessageBox.warning(self, "Erro", "N√£o foi poss√≠vel carregar o arquivo.")
            return
        
        from PySide6.QtWidgets import QDialog, QTableWidget, QTableWidgetItem
        
        preview_dialog = QDialog(self)
        preview_dialog.setWindowTitle("Preview dos Dados")
        preview_dialog.setMinimumSize(900, 500)
        
        layout = QVBoxLayout(preview_dialog)
        
        info_label = QLabel(f"üìä Mostrando {min(10, len(df))} de {len(df)} linhas | {len(df.columns)} colunas")
        info_label.setStyleSheet("font-weight: bold; padding: 10px;")
        layout.addWidget(info_label)
        
        table = QTableWidget()
        table.setRowCount(min(10, len(df)))
        table.setColumnCount(len(df.columns))
        table.setHorizontalHeaderLabels(df.columns.tolist())
        
        for i in range(min(10, len(df))):
            for j, col in enumerate(df.columns):
                value = str(df.iloc[i, j])
                if len(value) > 100:
                    value = value[:100] + "..."
                table.setItem(i, j, QTableWidgetItem(value))
        
        table.resizeColumnsToContents()
        layout.addWidget(table)
        
        btn_close = QPushButton("Fechar")
        btn_close.clicked.connect(preview_dialog.accept)
        layout.addWidget(btn_close)
        
        preview_dialog.exec()
        
    except Exception as e:
        QMessageBox.critical(self, "Erro", f"Erro ao gerar preview: {e}")
```

---

### 9. Aus√™ncia de Hist√≥rico de Processamentos
**Status Atual:** N√£o implementado  
**Problema:** Usu√°rio n√£o v√™ processamentos anteriores  
**Impacto:** Dif√≠cil rastrear trabalho feito

**Solu√ß√£o:**

```python
# core/processing_history.py (novo arquivo)

import json
from datetime import datetime
from pathlib import Path

class ProcessingHistory:
    def __init__(self, history_file='logs/history.json'):
        self.history_file = Path(history_file)
        self.history_file.parent.mkdir(exist_ok=True)
        self.load()
    
    def load(self):
        try:
            if self.history_file.exists():
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    self.entries = json.load(f)
            else:
                self.entries = []
        except:
            self.entries = []
    
    def add(self, file_path, model, prompt_type, duration, success, output_files=None):
        entry = {
            'timestamp': datetime.now().isoformat(),
            'file': Path(file_path).name,
            'model': model,
            'prompt': prompt_type,
            'duration_seconds': duration,
            'success': success,
            'output_files': output_files or []
        }
        self.entries.insert(0, entry)  # Mais recente primeiro
        self.entries = self.entries[:50]  # Manter √∫ltimos 50
        self.save()
    
    def save(self):
        try:
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(self.entries, f, indent=2, ensure_ascii=False)
        except:
            pass
    
    def get_recent(self, limit=10):
        return self.entries[:limit]

# ui/main_window.py - Integrar

from core.processing_history import ProcessingHistory

def __init__(self):
    # ... c√≥digo existente ...
    self.history = ProcessingHistory()

# Adicionar bot√£o no status bar:
btn_history = QPushButton("üìú Hist√≥rico")
btn_history.clicked.connect(self.show_history)
self.status_bar.addWidget(btn_history)

# Modificar on_processing_finished para salvar no hist√≥rico:
def on_processing_finished(self):
    # ... c√≥digo existente ...
    
    # Salvar no hist√≥rico
    self.history.add(
        file_path=self.file_path_input.text(),
        model=self.combo_model.currentText(),
        prompt_type=self.combo_prompt_type.currentText(),
        duration=self._elapsed_seconds,
        success=True,
        output_files=self._last_generated_files
    )

def show_history(self):
    from PySide6.QtWidgets import QDialog, QListWidget, QListWidgetItem
    
    dialog = QDialog(self)
    dialog.setWindowTitle("Hist√≥rico de Processamentos")
    dialog.setMinimumSize(700, 500)
    
    layout = QVBoxLayout(dialog)
    
    list_widget = QListWidget()
    
    for entry in self.history.get_recent(20):
        dt = datetime.fromisoformat(entry['timestamp'])
        mins, secs = divmod(entry['duration_seconds'], 60)
        
        status_icon = "‚úÖ" if entry['success'] else "‚ùå"
        text = (
            f"{status_icon} {dt.strftime('%d/%m/%Y %H:%M')} | "
            f"{entry['file']} | {entry['model']} | "
            f"Dura√ß√£o: {mins:02d}:{secs:02d}"
        )
        
        item = QListWidgetItem(text)
        item.setData(Qt.UserRole, entry)
        list_widget.addItem(item)
    
    list_widget.itemDoubleClicked.connect(lambda item: self._open_history_output(item.data(Qt.UserRole)))
    
    layout.addWidget(QLabel("üìä √öltimos 20 processamentos (clique duplo para abrir)"))
    layout.addWidget(list_widget)
    
    btn_close = QPushButton("Fechar")
    btn_close.clicked.connect(dialog.accept)
    layout.addWidget(btn_close)
    
    dialog.exec()

def _open_history_output(self, entry):
    if entry.get('output_files'):
        try:
            os.startfile(entry['output_files'][0])
        except:
            QMessageBox.warning(self, "Aviso", "Arquivo n√£o encontrado.")
```

---

## üü° M√âDIA PRIORIDADE - Performance

### 10. Processamento Sequencial (Sem Paraleliza√ß√£o)
**Status Atual:** Processa uma linha por vez  
**Problema:** Lento para arquivos grandes  
**Impacto:** 10 linhas = ~15 minutos

**Solu√ß√£o:**

```python
# core/worker_engine.py - Adicionar processamento paralelo

from concurrent.futures import ThreadPoolExecutor, as_completed

def process_file(self, file_path, model_override=None, rows_to_process=None, 
                 prompt_type_override=None, max_workers=3):
    import pandas as pd
    self.log_and_progress(f"Lendo arquivo: {file_path}")
    
    df, items = self.load_data(file_path)
    if df is None: return []

    generated_files = []
    config = {
        'model': model_override or 'gemini-1.5-pro',
        'temperature': 0.1
    }

    total = len(df)
    self.log_and_progress(f"Iniciando processamento paralelo de {total} linhas (workers: {max_workers})...")

    def process_single_row(row_idx, row):
        """Processa uma √∫nica linha"""
        if self.check_cancellation and self.check_cancellation():
            return None
        
        # Name Prefix
        prefix = f"Row_{row_idx}"
        for col in df.columns:
            if any(c in str(col).lower() for c in ['nome da empresa', 'empresa', 'company']):
                prefix = str(row[col])
                break
        
        self.log_and_progress(f"--- Processando: {prefix} ({row_idx+1}/{total}) ---")
        
        # Determine Prompt
        if prompt_type_override and "Autom√°tico" not in prompt_type_override:
            p_type = prompt_type_override
        else:
            p_type = "revenue"
            for c in df.columns:
                if "modelo" in str(c).lower():
                    p_type = str(row[c]).strip()
                    break
        
        # Load Prompt
        prompt_text = self.load_agent_prompt(p_type)
        if not prompt_text:
            self.log_and_progress(f"Prompt n√£o encontrado para '{p_type}'. Pulando.", "error")
            return None
        
        # Call AI
        full_prompt = f"{prompt_text}\n\nDADOS DO CLIENTE:\n{row.to_string()}"
        response = self.call_ai_api(full_prompt, config)
        
        if not response:
            self.log_and_progress("Falha na gera√ß√£o da IA.", "error")
            return None
        
        # Parse & Save
        parsed = self.parse_response(response)
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        rpt = self.generate_word_report(parsed, p_type, config['model'], timestamp, prefix, row_data=row)
        
        if rpt:
            self.log_and_progress(f"‚úÖ Relat√≥rio gerado: {prefix}", "success")
            return rpt
        return None

    # Processar em paralelo
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {}
        
        for row_idx, row in df.iterrows():
            if rows_to_process and row_idx not in rows_to_process:
                continue
            
            future = executor.submit(process_single_row, row_idx, row)
            futures[future] = row_idx
        
        for future in as_completed(futures):
            row_idx = futures[future]
            try:
                result = future.result()
                if result:
                    generated_files.append(result)
            except Exception as e:
                self.log_and_progress(f"‚ùå Erro na linha {row_idx}: {e}", "error")

    self.log_and_progress(f"Processamento finalizado. {len(generated_files)} arquivos gerados.")
    return generated_files
```

**Nota:** Cuidado com rate limits! Ajustar `max_workers` conforme tier da API.

---

### 11. Logs Sem Rota√ß√£o
**Status Atual:** Arquivo `worker.log` cresce indefinidamente  
**Problema:** Pode ocupar muito espa√ßo em disco  
**Impacto:** Logs gigantes ap√≥s meses de uso

**Solu√ß√£o:**

```python
# core/worker_engine.py - Modificar _setup_logging

from logging.handlers import RotatingFileHandler

def _setup_logging(self):
    logger = logging.getLogger("WorkerEngine")
    logger.setLevel(logging.INFO)
    
    if logger.handlers:
        logger.handlers.clear()
    
    # Rota√ß√£o: 10MB por arquivo, manter 5 backups
    fh = RotatingFileHandler(
        os.path.join(self.log_dir, 'worker.log'),
        maxBytes=10*1024*1024,  # 10MB
        backupCount=5,
        encoding='utf-8'
    )
    
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    fh.setFormatter(formatter)
    logger.addHandler(fh)
    
    return logger
```

---

### 12. Cache de Prompts Apenas em Mem√≥ria
**Status Atual:** `@lru_cache` perde dados ao fechar app  
**Problema:** Re-download de prompts do GitHub a cada execu√ß√£o  
**Impacto:** Lentid√£o e uso de banda

**Solu√ß√£o:** J√° est√° parcialmente resolvido (prompts s√£o salvos localmente ap√≥s download). Melhorar limpeza de cache expirado:

```python
# core/worker_engine.py - Adicionar m√©todo

def cleanup_old_caches(self):
    """Remove caches antigos para liberar espa√ßo"""
    if hasattr(self, 'response_cache'):
        count = self.response_cache.clear_expired()
        if count > 0:
            self.log_and_progress(f"üóëÔ∏è {count} caches expirados removidos", "debug")
```

---

## üü¢ BAIXA PRIORIDADE - DevOps & CI/CD

### 13. Aus√™ncia de `justfile`
**Status Atual:** N√£o implementado  
**Problema:** Comandos n√£o padronizados  
**Impacto:** Onboarding mais lento

**Solu√ß√£o:**

```makefile
# justfile (criar na raiz)

# Lista todos os comandos dispon√≠veis
default:
    @just --list

# Executa a aplica√ß√£o
run:
    python Xalq.py

# Executa os testes
test:
    pytest tests/ -v --cov=core --cov=ui --cov-report=html

# Instala depend√™ncias
install:
    pip install -r requirements.txt

# Lint do c√≥digo
lint:
    flake8 core/ ui/ tests/ --max-line-length=120 --exclude=__pycache__

# Formata c√≥digo
format:
    black core/ ui/ tests/ --line-length=120

# Limpa arquivos tempor√°rios
clean:
    rm -rf __pycache__ .pytest_cache .coverage htmlcov
    rm -rf logs/*.log
    find . -type d -name "__pycache__" -exec rm -rf {} +

# Executa linter + testes
check: lint test

# Build (futuro)
build:
    @echo "Build n√£o implementado ainda"
```

---

### 14. Aus√™ncia de CI/CD
**Status Atual:** N√£o implementado  
**Problema:** Sem automa√ß√£o de testes  
**Impacto:** Bugs podem passar despercebidos

**Solu√ß√£o:**

```yaml
# .github/workflows/ci.yml

name: CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: windows-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov flake8
    
    - name: Lint with flake8
      run: |
        flake8 core/ ui/ --max-line-length=120 --exclude=__pycache__
    
    - name: Run tests
      run: |
        pytest tests/ -v --cov=core --cov=ui --cov-report=xml
    
    - name: Upload coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

---

### 15. Depend√™ncias Sem Vers√£o Pinada
**Status Atual:** Vers√µes n√£o fixadas em `requirements.txt`  
**Problema:** Atualiza√ß√µes podem quebrar compatibilidade  
**Impacto:** Bugs inesperados em produ√ß√£o

**Solu√ß√£o:**

```txt
# requirements.txt (atualizar com vers√µes espec√≠ficas)

PySide6==6.6.1
pandas==2.2.0
openpyxl==3.1.2
requests==2.31.0
google-generativeai==0.3.2
python-docx==1.1.0
python-dotenv==1.0.0
keyring==24.3.0
tenacity==8.2.3

# Dev dependencies (criar requirements-dev.txt)
pytest==7.4.3
pytest-cov==4.1.0
flake8==6.1.0
black==23.12.1
```

---

## üü¢ BAIXA PRIORIDADE - Observabilidade

### 16. Aus√™ncia de M√©tricas de Performance da API
**Status Atual:** N√£o implementado  
**Problema:** Sem dados sobre lat√™ncia, taxa de sucesso  
**Impacto:** Dif√≠cil otimizar performance

**Solu√ß√£o:**

```python
# core/api_metrics.py (novo arquivo)

import time
from collections import defaultdict

class APIMetrics:
    def __init__(self):
        self.metrics = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_tokens': 0,
            'total_cost': 0.0,
            'latencies': [],
            'errors_by_type': defaultdict(int)
        }
    
    def record_request(self, success, latency, tokens=0, cost=0.0, error_type=None):
        self.metrics['total_requests'] += 1
        
        if success:
            self.metrics['successful_requests'] += 1
            self.metrics['latencies'].append(latency)
            self.metrics['total_tokens'] += tokens
            self.metrics['total_cost'] += cost
        else:
            self.metrics['failed_requests'] += 1
            if error_type:
                self.metrics['errors_by_type'][error_type] += 1
    
    def get_summary(self):
        if self.metrics['total_requests'] == 0:
            return None
        
        success_rate = (self.metrics['successful_requests'] / self.metrics['total_requests']) * 100
        avg_latency = sum(self.metrics['latencies']) / len(self.metrics['latencies']) if self.metrics['latencies'] else 0
        
        return {
            'total_requests': self.metrics['total_requests'],
            'success_rate': f"{success_rate:.1f}%",
            'avg_latency': f"{avg_latency:.2f}s",
            'total_cost': f"${self.metrics['total_cost']:.2f}",
            'total_tokens': self.metrics['total_tokens'],
            'top_errors': dict(sorted(self.metrics['errors_by_type'].items(), key=lambda x: x[1], reverse=True)[:3])
        }

# core/worker_engine.py - Integrar

from core.api_metrics import APIMetrics

def __init__(self, ...):
    # ... c√≥digo existente ...
    self.api_metrics = APIMetrics()

def call_ai_api(self, prompt_content, config):
    start_time = time.time()
    
    try:
        # ... chamada API ...
        
        latency = time.time() - start_time
        tokens = 0
        cost = 0.0
        
        if hasattr(response, 'usage_metadata') and response.usage_metadata:
            tokens = response.usage_metadata.prompt_token_count + response.usage_metadata.candidates_token_count
            # cost j√° calculado pelo CostTracker
        
        self.api_metrics.record_request(
            success=True,
            latency=latency,
            tokens=tokens,
            cost=cost
        )
        
        return response.text
        
    except Exception as e:
        latency = time.time() - start_time
        error_type = type(e).__name__
        self.api_metrics.record_request(success=False, latency=latency, error_type=error_type)
        raise

# Adicionar m√©todo para exibir m√©tricas ao final:
def show_session_metrics(self):
    summary = self.api_metrics.get_summary()
    if summary:
        self.log_and_progress("=" * 50, "info")
        self.log_and_progress("üìä M√âTRICAS DA SESS√ÉO", "info")
        self.log_and_progress(f"Total de requisi√ß√µes: {summary['total_requests']}", "info")
        self.log_and_progress(f"Taxa de sucesso: {summary['success_rate']}", "info")
        self.log_and_progress(f"Lat√™ncia m√©dia: {summary['avg_latency']}", "info")
        self.log_and_progress(f"Custo total: {summary['total_cost']}", "info")
        self.log_and_progress(f"Tokens processados: {summary['total_tokens']}", "info")
        if summary['top_errors']:
            self.log_and_progress(f"Erros principais: {summary['top_errors']}", "error")
        self.log_and_progress("=" * 50, "info")
```

---

## üìã Checklist de Implementa√ß√£o Priorizado

### Sprint 1 - Funda√ß√£o Cr√≠tica (2 semanas) üî¥
**Foco:** Custos, Testes, Valida√ß√£o

- [ ] #4: Monitoramento de custos API
- [ ] #5: Cache de respostas API
- [ ] #6: Rate limiting
- [ ] #1: Expandir cobertura de testes (70%)
- [ ] #2: Valida√ß√£o de resposta da API

**Impacto:** Economia de $45/m√™s + visibilidade financeira + qualidade

---

### Sprint 2 - UX Essencial (2 semanas) üü°
**Foco:** Produtividade do Usu√°rio

- [ ] #7: Busca e filtros em logs
- [ ] #8: Preview de dados
- [ ] #9: Hist√≥rico de processamentos
- [ ] #3: Refinar tratamento de erros

**Impacto:** Redu√ß√£o de 50% no tempo de debug

---

### Sprint 3 - Performance (1-2 semanas) üü°
**Foco:** Velocidade

- [ ] #10: Processamento paralelo (cuidado com rate limits!)
- [ ] #11: Rota√ß√£o de logs
- [ ] #12: Limpeza de caches expirados

**Impacto:** Redu√ß√£o de 60% no tempo de processamento (com workers adequados)

---

### Sprint 4 - DevOps & Observabilidade (1 semana) üü¢
**Foco:** Infraestrutura

- [ ] #13: Criar justfile
- [ ] #14: Configurar CI/CD
- [ ] #15: Pinar vers√µes de depend√™ncias
- [ ] #16: M√©tricas de performance da API

**Impacto:** Automa√ß√£o e confiabilidade

---

## üí∞ An√°lise de ROI

### Economia Mensal Estimada (100 processamentos/m√™s)

| Melhoria | Economia | Tipo |
|----------|----------|------|
| Cache de Respostas (#5) | $45/m√™s | üíµ Financeira |
| Rate Limiting (#6) | $15/m√™s | üíµ Evita bloqueios |
| Monitoramento (#4) | $25/m√™s | üíµ Otimiza√ß√£o |
| Busca em Logs (#7) | 10h/m√™s | ‚è±Ô∏è Tempo |
| Preview (#8) | 5h/m√™s | ‚è±Ô∏è Evita erros |
| Hist√≥rico (#9) | 3h/m√™s | ‚è±Ô∏è Rastreamento |
| Processamento Paralelo (#10) | 20h/m√™s | ‚è±Ô∏è Velocidade |
| **TOTAL** | **$85/m√™s + 38h/m√™s** | **$1,905/m√™s*** |

*Considerando $30/hora de trabalho t√©cnico + economia direta de API

**Payback:** 1-2 sprints de desenvolvimento

---

## üéØ M√©tricas de Sucesso

| M√©trica | Atual | Meta Sprint 1 | Meta Sprint 4 |
|---------|-------|---------------|---------------|
| Cobertura de Testes | ~5% | 40% | 70% |
| Custo Rastreado | 0% | 100% | 100% |
| Taxa de Cache Hit | 0% | 30% | 40% |
| Tempo Processamento (10 linhas) | ~15min | ~15min | ~6min |
| Tempo para Debug | ~5min | ~2min | ~30s |
| Incidentes Rate Limit | Desconhecido | 0 | 0 |
| Satisfa√ß√£o Usu√°rio (NPS) | N/A | 7/10 | 8/10 |

---

## üöÄ Melhorias Futuras (Backlog)

### Fase 2 - Recursos Avan√ßados

17. **Notifica√ß√µes Desktop** - Alertar quando processamento concluir
18. **Autocomplete em Combos** - Busca r√°pida em listas grandes
19. **Sistema de Temas** - Modo claro/escuro
20. **Atalhos de Teclado** - Ctrl+O, F5, Esc
21. **Tooltips Informativos** - Ajuda contextual
22. **Estimativa de Custo Pr√©-Processamento** - Transpar√™ncia financeira
23. **Streaming de Respostas** - Feedback em tempo real
24. **Fallback Inteligente de Modelos** - Pro ‚Üí Flash ‚Üí Reduzir prompt
25. **Internacionaliza√ß√£o (i18n)** - Suporte a m√∫ltiplos idiomas
26. **Dockerfile** - Containeriza√ß√£o
27. **Logs Estruturados (JSON)** - An√°lise automatizada
28. **Dashboard de M√©tricas** - Visualiza√ß√£o de performance
29. **Backup Autom√°tico** - Prote√ß√£o de dados
30. **API REST** - Integra√ß√£o com outros sistemas
31. **Modo Batch** - Processamento via CLI
32. **Webhooks** - Notifica√ß√µes externas

---

## üìö Refer√™ncias T√©cnicas

- [Gemini API Docs](https://ai.google.dev/docs)
- [PySide6 Documentation](https://doc.qt.io/qtforpython/)
- [Tenacity (Retry)](https://tenacity.readthedocs.io/)
- [Keyring (Secure Storage)](https://pypi.org/project/keyring/)
- [Pytest Best Practices](https://docs.pytest.org/en/stable/goodpractices.html)
- [Python Logging Cookbook](https://docs.python.org/3/howto/logging-cookbook.html)

---

## üîç Notas de Implementa√ß√£o

### J√° Implementado (N√£o Refazer)

‚úÖ **Seguran√ßa B√°sica:**
- Sanitiza√ß√£o de API keys em logs (parcial)
- Keyring para armazenamento seguro
- Sanitiza√ß√£o de nomes de arquivo

‚úÖ **Resili√™ncia:**
- Retry com backoff exponencial (tenacity)
- Fallback entre modelos
- Timeout configur√°vel (600s)
- Cancelamento de processamento

‚úÖ **UI Moderna:**
- Drag & Drop
- Bot√£o cancelar
- Monitor de recursos
- Timer elapsed
- Splash screen

‚úÖ **Testes Iniciais:**
- test_sanitize_filename_basics
- test_sanitize_filename_injection
- test_log_sanitization

### Pontos de Aten√ß√£o

‚ö†Ô∏è **Rate Limiting vs Paraleliza√ß√£o:**
- Gemini Free: 15 RPM
- Gemini Paid: 360 RPM
- Ajustar `max_workers` conforme tier

‚ö†Ô∏è **Cache:**
- TTL padr√£o: 7 dias
- Limpar caches expirados periodicamente
- Considerar tamanho em disco

‚ö†Ô∏è **Custos:**
- gemini-3-pro: $0.0225 por 3k tokens
- gemini-2.5-flash: $0.0007 por 3k tokens (97% mais barato!)
- Usar Flash para testes, Pro para produ√ß√£o

‚ö†Ô∏è **Testes:**
- Mockar chamadas de API
- Usar fixtures para dados de teste
- Evitar testes que dependem de rede

---

## üìä Resumo Final

**Projeto:** XALQ Agent v1.2.0  
**Status:** Base s√≥lida, pronto para otimiza√ß√µes  
**Prioridade #1:** Controle de custos API  
**Prioridade #2:** Expandir testes  
**Prioridade #3:** UX avan√ßada

**Pr√≥ximos Passos:**
1. Implementar Sprint 1 (2 semanas)
2. Validar economia de custos
3. Coletar feedback de usu√°rios
4. Iterar com Sprint 2

**√öltima Atualiza√ß√£o:** 12/02/2026  
**Respons√°vel:** An√°lise Completa do C√≥digo-Fonte
